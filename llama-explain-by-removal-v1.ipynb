{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9209219,"sourceType":"datasetVersion","datasetId":5568393}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U bitsandbytes\n!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:09:50.606914Z","iopub.execute_input":"2024-08-27T07:09:50.608120Z","iopub.status.idle":"2024-08-27T07:10:17.609516Z","shell.execute_reply.started":"2024-08-27T07:09:50.608065Z","shell.execute_reply":"2024-08-27T07:10:17.608345Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport time\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport shap\nimport numpy as np\nimport torch.nn.functional as F\nimport pandas as pd\nfrom time import time\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"The attention mask and the pad token id were not set.*\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:10:52.100240Z","iopub.execute_input":"2024-08-27T07:10:52.100693Z","iopub.status.idle":"2024-08-27T07:10:55.922708Z","shell.execute_reply.started":"2024-08-27T07:10:52.100652Z","shell.execute_reply":"2024-08-27T07:10:55.921862Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Llama:\n    \n    def __init__(self, model, tokenizer):\n        \n        self.model = model\n        self.tokenizer = tokenizer\n        \n    \n    def single_predict(self,token):\n#         token = torch.tensor(train_v1[token_input_column][0:1].values)\n#         mask = torch.tensor(train_v1[token_att_column][0:1].values)\n        \n\n        with torch.no_grad():\n            output_ids = model.generate(\n                token,\n                max_length=token.shape[1] + 3,\n                num_return_sequences=1,\n                do_sample=False,\n                return_dict_in_generate=True, output_scores=True\n            )\n        \n        A_logit =F.softmax(output_ids.scores[0])[0][32]\n        B_logit =F.softmax(output_ids.scores[0])[0][33]\n        \n        max_index = np.argmax(output_ids.scores[0])\n#         return [A_logit,B_logit,max_index]\n        return A_logit\n    \n    def predict(self, token_arr):\n\n        output_list = []\n\n        for i in range(token_arr.shape[0]):\n            output_list.append(self.single_predict(token_arr[i:i+1,:]))\n\n        return np.array(output_list)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:10:55.924302Z","iopub.execute_input":"2024-08-27T07:10:55.924794Z","iopub.status.idle":"2024-08-27T07:10:55.934347Z","shell.execute_reply.started":"2024-08-27T07:10:55.924742Z","shell.execute_reply":"2024-08-27T07:10:55.933383Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def truncate_text(text, max_tokens=256):\n    tokens = text.split()\n\n    truncated_tokens = tokens[:max_tokens]\n\n    truncated_text = ' '.join(truncated_tokens)\n\n    return truncated_text\n\ndef prompt_generation(tweet):\n    trucated_tweet = truncate_text(tweet)\n\n    prompt = f'''Please select the option (A or B) that most closely describes the following claim: {trucated_tweet}.\n    (A) True\n    (B) False\n    Choice: ('''\n\n    #     print(prompt)\n\n    return prompt\n\ndef generate_token(input_text):\n    # Tokenize the input text\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n    # Generate the output\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids,\n            max_length=80,\n            num_return_sequences=1,\n            do_sample=False,\n            return_dict_in_generate=True, output_scores=True\n        )\n\n    index = np.argmax(F.softmax(output_ids.scores[0]))\n    token_probability = F.softmax(output_ids.scores[0])[0][index]\n    token = tokenizer.convert_ids_to_tokens([index])[0]\n    return [index, token, token_probability, output_ids]\n\ndef predict_label(tweet):\n    prompt = prompt_generation(tweet)\n    print(prompt)\n    lable = generate_token(prompt)\n    return lable\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:10:55.935607Z","iopub.execute_input":"2024-08-27T07:10:55.936296Z","iopub.status.idle":"2024-08-27T07:10:55.955177Z","shell.execute_reply.started":"2024-08-27T07:10:55.936236Z","shell.execute_reply":"2024-08-27T07:10:55.954340Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def crossentropyloss(pred, target):\n    '''Cross entropy loss that does not average across samples.'''\n    if pred.ndim == 1:\n        pred = pred[:, np.newaxis]\n        pred = np.concatenate((1 - pred, pred), axis=1)\n\n    if pred.shape == target.shape:\n        # Soft cross entropy loss.\n        pred = np.clip(pred, a_min=1e-12, a_max=1-1e-12)\n        return - np.sum(np.log(pred) * target, axis=1)\n    else:\n        # Standard cross entropy loss.\n        return - np.log(pred[np.arange(len(pred)), target])\n\nclass DatasetLossGame:\n    '''\n    Cooperative game representing the model's loss over a dataset.\n\n    TODO: this implementation is slower than SAGE because it averages\n    loss over entire dataset for each S. Need to reimplement as a stochastic\n    game (with caching) to accelerate convergence.\n\n    Args:\n      extension: model extension (see removal.py).\n      data: array of model inputs.\n      labels: array of corresponding labels.\n      loss: loss function (see utils.py).\n    '''\n    def __init__(self, extension, data, labels, loss):\n        # Convert labels dtype if necessary.\n        if loss is crossentropyloss:\n            # Make sure not soft cross entropy.\n            if (labels.ndim == 1) or (labels.shape[1] == 1):\n                # Only convert if float.\n                if np.issubdtype(labels.dtype, np.floating):\n                    labels = labels.astype(int)\n\n        self.extension = extension\n        self.data = data\n        self.labels = labels\n        self.loss = loss\n        self.players = data.shape[1]\n        self.data_tile = self.data\n        self.label_tile = self.labels\n\n    def __call__(self, S):\n        # Return scalar is single subset.\n        single_eval = (S.ndim == 1)\n        if single_eval:\n            S = S[np.newaxis]\n\n        # Prepare data.\n        if len(self.data_tile) != len(self.data) * len(S):\n            self.data_tile = np.tile(self.data, (len(S), 1))\n            self.label_tile = np.tile(\n                self.labels,\n                (len(S), *[1 for _ in range(len(self.labels.shape[1:]))]))\n        S = S.repeat(len(self.data), 0)\n\n        # Evaluate.\n        output = - self.loss(self.extension(self.data_tile, S), self.label_tile)\n        output = output.reshape((-1, self.data.shape[0]))\n        output = np.mean(output, axis=1)\n        if single_eval:\n            output = output[0]\n        return output\n\ndef default_batch_size(game):\n    '''\n    Determine batch size.\n\n    TODO maybe consider the number of features, or the type of model extension.\n    '''\n    if isinstance(game, DatasetLossGame):\n        return 32\n    else:\n        return 512\n\nclass MarginalExtension:\n    '''Extend a model by marginalizing out removed features using their\n    marginal distribution.'''\n\n    def __init__(self, data, model):\n        self.model = model\n        self.data = data\n        # self.data_repeat = data\n        self.samples = len(data)\n        # self.x_addr = None\n        # self.x_repeat = None\n\n    def __call__(self, x, S):\n        # Prepare x and S.\n        n = len(x)\n        x = x.repeat(self.samples, 0)\n        S = S.repeat(self.samples, 0)\n        # if self.x_addr != id(x):\n        #     self.x_addr = id(x)\n        #     self.x_repeat = x.repeat(self.samples, 0)\n        # x = self.x_repeat\n\n        # Prepare samples.\n        # if len(self.data_repeat) != self.samples * n:\n        self.data_repeat = np.tile(self.data.values[:,0:S.shape[1]], (n, 1))\n\n        # Replace specified indices.\n        x_ = x.copy()\n        x_[~S] = self.data_repeat[~S]\n        \n#         return x_\n\n        # Make predictions.\n        pred = self.model.predict(torch.tensor(x_))\n        \n        return pred;\n        pred = pred.reshape(-1, self.samples, *pred.shape[1:])\n        return np.mean(pred, axis=1)\n\nclass PredictionGame:\n    '''\n    Cooperative game for an individual example's prediction.\n\n    Args:\n      extension: model extension (see removal.py).\n      sample: numpy array representing a single model input.\n    '''\n\n    def __init__(self, extension, sample, input_col, att_col):\n        # Add batch dimension to sample.\n        if sample.ndim == 1:\n            sample = sample[np.newaxis]\n        elif sample.shape[0] != 1:\n            raise ValueError('sample must have shape (ndim,) or (1,ndim)')\n\n        self.extension = extension\n        self.sample = sample\n        self.input_col = input_col\n        self.att_col = att_col\n\n\n        self.players = sample[att_col].sum(axis = 1)[0]\n\n        # Caching.\n        self.sample_repeat = sample\n\n    def __call__(self, S):\n        # Return scalar if single subset.\n        single_eval = (S.ndim == 1)\n        if single_eval:\n            S = S[np.newaxis]\n            input_data = self.sample\n        else:\n            # Try to use caching for repeated data.\n            if len(S) != len(self.sample_repeat):\n                self.sample_repeat = self.sample[self.input_col].values[:,0:self.players].repeat(len(S), 0)\n            input_data = self.sample_repeat\n\n        # Evaluate.\n        output = self.extension(input_data, S)\n        if single_eval:\n            output = output[0]\n        return output\n\ndef RemoveIndividual(game, batch_size=None):\n    '''Calculate feature attributions by removing individual\n    players from the grand coalition.'''\n    if batch_size is None:\n        batch_size = default_batch_size(game)\n\n    # Setup.\n    n = game.players\n    S = np.ones((n + 1, n), dtype=bool)\n    for i in range(n):\n        S[i + 1, i] = 0\n\n    # Evaluate.\n    output_list = []\n    for i in range(int(np.ceil(len(S) / batch_size))):\n        output_list.append(game(S[i * batch_size:(i + 1) * batch_size]))\n    output = np.concatenate(output_list, axis=0)\n    \n#     return output\n\n    return output[0] - output[1:]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:10:55.957090Z","iopub.execute_input":"2024-08-27T07:10:55.957397Z","iopub.status.idle":"2024-08-27T07:10:55.986669Z","shell.execute_reply.started":"2024-08-27T07:10:55.957364Z","shell.execute_reply":"2024-08-27T07:10:55.985701Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\naccess_token = user_secrets.get_secret(\"hf_access_code\")\n\nprint(access_token)\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=access_token)\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    token=access_token,\n    quantization_config=quantization_config\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:10:55.987726Z","iopub.execute_input":"2024-08-27T07:10:55.988033Z","iopub.status.idle":"2024-08-27T07:13:37.212233Z","shell.execute_reply.started":"2024-08-27T07:10:55.988001Z","shell.execute_reply":"2024-08-27T07:13:37.211402Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"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\n","output_type":"stream"},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ef59432f4a45bcb4ddac8989ebe480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b5a42242b84c7db8f8a5f728ecfb82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ce46b279bf4bf1b1438755562a80cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59e8bfd70c1e4d0abf149c8761f7125a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506344bf55db4a28bc4f37f61b8b9dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66eefa25e17447979e89f6cfaeec58f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc265d09f8d46e89d2703a34df5e6b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7b3759b16e465fb4671dc309b59ffb"}},"metadata":{}}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport logging\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:13:37.213297Z","iopub.execute_input":"2024-08-27T07:13:37.213740Z","iopub.status.idle":"2024-08-27T07:13:37.218877Z","shell.execute_reply.started":"2024-08-27T07:13:37.213706Z","shell.execute_reply":"2024-08-27T07:13:37.217763Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"text_to_analyse = 50\n\nsample_to_replace = [5,2]\n\n\nllama_model = Llama(model, tokenizer)\n\ntrain = pd.read_csv(\"/kaggle/input/fake-new-covid/Constraint_Train.csv\")\ntrain['prompt'] = train[\"tweet\"].apply(lambda x: prompt_generation(x))\nsentences = list(train[\"prompt\"])\n\n\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize the sentences with padding\nencoded_inputs = tokenizer(\n    sentences,\n    padding=True,  # Pads all sequences to the same length\n    return_tensors=\"pt\"  # Returns PyTorch tensors\n)\n\n\ntoken_input_ids = encoded_inputs[\"input_ids\"]\ntoken_attention_mask = encoded_inputs[\"attention_mask\"]\n\ntoken_input_column  = [f\"col_input_{i}\" for i in range(token_input_ids.shape[1])]\ntoken_att_column  = [f\"col_att_{i}\" for i in range(token_attention_mask.shape[1])]\n\ntoken_input_df = pd.DataFrame(token_input_ids, columns = token_input_column)\ntoken_att_df = pd.DataFrame(token_attention_mask, columns = token_att_column)\n\ntrain_v1 = pd.concat([train,token_input_df,token_att_df], axis = 1)\n\nLlama_v1 = Llama(model, tokenizer)\n\nremoval = MarginalExtension(\n    train_v1[token_input_column +token_att_column][9:13].reset_index(drop = True),\n    Llama_v1\n)\nbehavioural = PredictionGame(removal,\n                             train_v1[token_input_column +token_att_column][text_to_analyse:text_to_analyse+1].reset_index(drop = True) ,\n                             token_input_column,\n                             token_att_column)\nsummary = RemoveIndividual(behavioural)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T08:11:26.720472Z","iopub.execute_input":"2024-08-27T08:11:26.721462Z","iopub.status.idle":"2024-08-27T08:16:53.193382Z","shell.execute_reply.started":"2024-08-27T08:11:26.721416Z","shell.execute_reply":"2024-08-27T08:16:53.192242Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"lenght_of_sentence = train_v1[text_to_analyse:text_to_analyse+1][token_att_column].values.sum()\n\nimportance = {}\nfor i in range(lenght_of_sentence):\n    importance[tokenizer.decode(train_v1[token_input_column].values[text_to_analyse:text_to_analyse+1,i]) ] =10*summary[i]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T08:16:53.195631Z","iopub.execute_input":"2024-08-27T08:16:53.196044Z","iopub.status.idle":"2024-08-27T08:16:53.588685Z","shell.execute_reply.started":"2024-08-27T08:16:53.195999Z","shell.execute_reply":"2024-08-27T08:16:53.587847Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Explaining the prediction ","metadata":{}},{"cell_type":"code","source":"import random\nfrom IPython.core.display import display, HTML\nimport matplotlib.pyplot as plt\n\n\ndef highlighter(word):\n    # Default color if the word is not in the importance dictionary\n    default_color = \"#FFFFFF\"  # White background\n    \n    # Get the importance value (with a default of 0 if not found)\n    importance_value = importance.get(word, 0)\n    \n    # Normalize the importance value to range [-1, 1] for a diverging color map\n    norm = plt.Normalize(vmin=-1, vmax=1)\n    \n    # Use a diverging colormap (e.g., coolwarm) to get the color based on importance\n    color = plt.cm.coolwarm(norm(importance_value))\n    \n    # Convert the color from RGBA to Hex\n    color_hex = \"#{:02x}{:02x}{:02x}\".format(int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n    \n    # Highlight the word with the corresponding color\n    word = f'<span style=\"background-color:{color_hex}\">' + word + '</span>'\n    \n    return word\n\n\nprint(\"Here Red color suggest positive importance and Blue color suggest negetive importance \\n\")\n\ntext = ''.join([ highlighter(tokenizer.decode(train_v1[token_input_column].values[text_to_analyse:text_to_analyse+1,i]) ) for i in range(71)])\n\ndisplay(HTML(text))  \n","metadata":{"execution":{"iopub.status.busy":"2024-08-27T09:12:40.343851Z","iopub.execute_input":"2024-08-27T09:12:40.344923Z","iopub.status.idle":"2024-08-27T09:12:40.622791Z","shell.execute_reply.started":"2024-08-27T09:12:40.344872Z","shell.execute_reply":"2024-08-27T09:12:40.621541Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Here Red color suggest positive importance and Blue color suggest negetive importance \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"background-color:#dddcdb\"><|begin_of_text|></span><span style=\"background-color:#dddcdb\">Please</span><span style=\"background-color:#dddcdb\"> select</span><span style=\"background-color:#dddcdb\"> the</span><span style=\"background-color:#dddcdb\"> option</span><span style=\"background-color:#d75444\"> (</span><span style=\"background-color:#d65243\">A</span><span style=\"background-color:#dddcdb\"> or</span><span style=\"background-color:#dddcdb\"> B</span><span style=\"background-color:#c22d31\">)</span><span style=\"background-color:#dddcdb\"> that</span><span style=\"background-color:#dddcdb\"> most</span><span style=\"background-color:#dddcdb\"> closely</span><span style=\"background-color:#dddcdb\"> describes</span><span style=\"background-color:#dddcdb\"> the</span><span style=\"background-color:#dddcdb\"> following</span><span style=\"background-color:#dddcdb\"> claim</span><span style=\"background-color:#f6a384\">:</span><span style=\"background-color:#dddcdb\"> Masks</span><span style=\"background-color:#dddcdb\"> can</span><span style=\"background-color:#dddcdb\"> help</span><span style=\"background-color:#dddcdb\"> prevent</span><span style=\"background-color:#dddcdb\"> the</span><span style=\"background-color:#dddcdb\"> spread</span><span style=\"background-color:#dddcdb\"> of</span><span style=\"background-color:#cc3f39\"> #</span><span style=\"background-color:#dddcdb\">COVID</span><span style=\"background-color:#dddcdb\">19</span><span style=\"background-color:#dddcdb\"> when</span><span style=\"background-color:#dddcdb\"> they</span><span style=\"background-color:#dddcdb\"> are</span><span style=\"background-color:#dddcdb\"> widely</span><span style=\"background-color:#dddcdb\"> used</span><span style=\"background-color:#dddcdb\"> in</span><span style=\"background-color:#dddcdb\"> public</span><span style=\"background-color:#dddcdb\">.</span><span style=\"background-color:#dddcdb\"> When</span><span style=\"background-color:#dddcdb\"> you</span><span style=\"background-color:#dddcdb\"> wear</span><span style=\"background-color:#dddcdb\"> a</span><span style=\"background-color:#dddcdb\"> mask</span><span style=\"background-color:#dddcdb\"> you</span><span style=\"background-color:#dddcdb\"> can</span><span style=\"background-color:#dddcdb\"> help</span><span style=\"background-color:#dddcdb\"> protect</span><span style=\"background-color:#dddcdb\"> those</span><span style=\"background-color:#dddcdb\"> around</span><span style=\"background-color:#dddcdb\"> you</span><span style=\"background-color:#dddcdb\">.</span><span style=\"background-color:#dddcdb\"> When</span><span style=\"background-color:#dddcdb\"> others</span><span style=\"background-color:#dddcdb\"> wear</span><span style=\"background-color:#dddcdb\"> one</span><span style=\"background-color:#dddcdb\"> they</span><span style=\"background-color:#dddcdb\"> can</span><span style=\"background-color:#dddcdb\"> help</span><span style=\"background-color:#dddcdb\"> protect</span><span style=\"background-color:#dddcdb\"> people</span><span style=\"background-color:#dddcdb\"> around</span><span style=\"background-color:#dddcdb\"> them</span><span style=\"background-color:#dddcdb\"> incl</span><span style=\"background-color:#dddcdb\">.</span><span style=\"background-color:#dddcdb\"> you</span><span style=\"background-color:#dddcdb\">.</span><span style=\"background-color:#f49d7e\"> https</span><span style=\"background-color:#eecfbe\">://</span><span style=\"background-color:#cdd9ec\">t</span><span style=\"background-color:#cdd9ec\">.co</span><span style=\"background-color:#dddcdb\">/j</span><span style=\"background-color:#dddcdb\">k</span><span style=\"background-color:#b30326\">W</span>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"token_for_text = torch.tensor(train_v1[token_input_column].values[text_to_analyse:text_to_analyse+1,:][:,0:lenght_of_sentence])\nwith torch.no_grad():\n    output_ids = model.generate(\n        token_for_text,\n        max_length=token_for_text.shape[1] + 3,\n        num_return_sequences=1,\n        do_sample=False,\n        return_dict_in_generate=True, output_scores=True\n    )\n        ","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:27:34.078609Z","iopub.execute_input":"2024-08-27T07:27:34.079349Z","iopub.status.idle":"2024-08-27T07:27:34.743982Z","shell.execute_reply.started":"2024-08-27T07:27:34.079304Z","shell.execute_reply":"2024-08-27T07:27:34.742910Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(output_ids.sequences[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-27T07:27:36.245809Z","iopub.execute_input":"2024-08-27T07:27:36.246634Z","iopub.status.idle":"2024-08-27T07:27:36.254178Z","shell.execute_reply.started":"2024-08-27T07:27:36.246585Z","shell.execute_reply":"2024-08-27T07:27:36.252944Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|>Please select the option (A or B) that most closely describes the following claim: States reported 1121 deaths a small rise from last Tuesday. Southern states reported 640 of those deaths. https://t.co/YASGRTT4ux.\\n    (A) True\\n    (B) False\\n    Choice: (A) True'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}